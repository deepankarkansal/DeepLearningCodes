# -*- coding: utf-8 -*-
"""DL_A4_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h1zKZ-yM0MyAYCkdXA8-1lD65BRlYZW0

# Mount the drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Libraries"""

import xml.etree.ElementTree as ET
import pandas as pd
import pandas as pd
import joblib
import keras
import tensorflow as tf
import os
import time
import math
import keras
from keras.models import Input, Model
from keras.layers import Dense, LSTM, Embedding
from keras.layers import Layer
import keras.backend as K
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras import layers
from keras.utils import np_utils
from sklearn.metrics import confusion_matrix
import seaborn as sns
from keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.constraints import max_norm
from sklearn import metrics
from scipy import interp
from keras.models import Model

import numpy as np
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import KeyedVectors
list_emotions = ['negative', 'neutral', 'positive']

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer 
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
!pip install contractions
import contractions
nltk.download('universal_tagset')
from nltk.stem.porter import PorterStemmer

"""# Make data"""

def makeDF(path):
  tree = ET.parse(path)
  root = tree.getroot()
  df=pd.DataFrame(columns=['context','target','output'])
  for sentence in root:
    for subelem in sentence:  
        if len(subelem.text) >13 :
          tt= subelem.text
        for subsub in subelem:
          for i in subsub.attrib :
            if i == 'term':
              newrow={'context' : tt,'target' : subsub.attrib['term'] , 'output' :  subsub.attrib['polarity']}
              df = df.append(newrow, ignore_index=True)
  return df

train_laptop = makeDF('/content/drive/MyDrive/DL/Assignment4/ABSA complete Dataset/ABSA Train/Laptops_Train.xml')
test_laptop = makeDF('/content/drive/MyDrive/DL/Assignment4/ABSA complete Dataset/ABSA Test/Laptops_Test_Gold.xml')
train_rest = makeDF('/content/drive/MyDrive/DL/Assignment4/ABSA complete Dataset/ABSA Train/Restaurants_Train.xml')
test_rest = makeDF('/content/drive/MyDrive/DL/Assignment4/ABSA complete Dataset/ABSA Test/Restaurants_Test_Gold.xml')
train_laptop.to_csv('/content/drive/MyDrive/DL/Assignment4/train_laptop.csv')
test_laptop.to_csv('/content/drive/MyDrive/DL/Assignment4/test_laptop.csv')
train_rest.to_csv('/content/drive/MyDrive/DL/Assignment4/train_rest.csv')
test_rest.to_csv('/content/drive/MyDrive/DL/Assignment4/test_rest.csv')

embeddings_dict = joblib.load('/content/drive/MyDrive/DL/Glove_Embedding')

embeddings_dict = joblib.load('/content/drive/MyDrive/ML/Group5_MLProject_EndSem/Glove_Embedding')

def max_length_sentence(data):
  # dictionary_final = []
  max = 0
  for s in data:
    words = preprocessing(s)
    if max < len(words):
      max = len(words)
    # for word in words:
    #   if word not in dictionary_final:
    #     dictionary_final.append(word)
  return max

max_sentence = max_length_sentence(train_rest['target'])
print(max_sentence)

max_sentence = max_length_sentence(train_rest['context'])
print(max_sentence)

max_sentence = max_length_sentence(test_rest['context'])
print(max_sentence)

max_sentence = max_length_sentence(train_laptop['target'])
print(max_sentence)

max_sentence = max_length_sentence(test_laptop['target'])
print(max_sentence)

max_sentence = max_length_sentence(train_laptop['context'])
print(max_sentence)

max_sentence = max_length_sentence(test_laptop['context'])
print(max_sentence)

"""# Data Loading and Other functions"""

# from imblearn.over_sampling import RandomOverSampler, SMOTE
# def equilising(data, data_target, y_t):
#   ros = RandomOverSampler(random_state = 42) 
#   X_res, y_res = ros.fit_resample(data, y_t)
#   X_res_target, _ = ros.fit_resample(data_target, y_t)
#   return X_res, X_res_target, y_res

# X_train_padded1, X_train_padded_target1, y_train_rest1 = equilising(X_train_padded, X_train_padded_target, y_train_rest)

rest_glove = joblib.load('/content/drive/MyDrive/DL/Assignment_4/restaurant_GloVe')
laptop_glove = joblib.load('/content/drive/MyDrive/DL/Assignment_4/laptop_GloVe')

laptop_glove_target = joblib.load('/content/drive/MyDrive/DL/Assignment_4/laptop_GloVe_target')
rest_glove_target = joblib.load('/content/drive/MyDrive/DL/Assignment_4/restaurant_GloVe_target')

train_laptop = pd.read_csv('/content/drive/MyDrive/DL/Assignment_4/train_laptop.csv')
test_laptop = pd.read_csv('/content/drive/MyDrive/DL/Assignment_4/test_laptop.csv')
train_rest = pd.read_csv('/content/drive/MyDrive/DL/Assignment_4/train_rest.csv')
test_rest = pd.read_csv('/content/drive/MyDrive/DL/Assignment_4/test_rest.csv')

def simplify(df,y):
  corpus = []
  for i in range(0, y.size):
    review = re.sub('[^a-zA-Z0-9]', ' ', str(df[i]))
    review = review.lower()
    review = review.split(' ')
    review = ' '.join(review)
    corpus.append(review)
  return corpus

def preprocessing(corpus_train, corpus_test, embeddings_dict, max_sentence_length=40):

  t = Tokenizer()
  t.fit_on_texts(corpus_train)
  vocab_size = len(t.word_index)+1
  X_train_encoded = t.texts_to_sequences(corpus_train)
  X_test_encoded = t.texts_to_sequences(corpus_test)
  X_train_padded = pad_sequences(X_train_encoded,maxlen=max_sentence_length,padding='post')
  X_test_padded = pad_sequences(X_test_encoded,maxlen=max_sentence_length,padding='post')
  embeddings_matrix = np.zeros((vocab_size,300))
  for word,i in t.word_index.items():
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None:
      embeddings_matrix[i] = embedding_vector
      
  return embeddings_matrix, X_train_padded, X_test_padded, vocab_size

"""# IAN, WithoutAttention and other helper functions  """

def plotting_epochs(training_, validation_, lossOrAccu, size=(6, 4)):
  plt.figure(figsize=size)
  plt.plot(training_, 'black', linewidth=2.0)
  plt.plot(validation_, 'blue', linewidth=2.0)
  plt.legend(['Training '+lossOrAccu, 'Validation '+lossOrAccu], fontsize=14)
  plt.xlabel('Epochs', fontsize=10)
  plt.ylabel(lossOrAccu, fontsize=10)
  plt.title(lossOrAccu+' Curves', fontsize=12)

class WithoutAttention(tf.keras.Model):

    def __init__(self, embeddings_matrix=None, embeddings_matrix_target=None, max_aspect_len=10):
        super(WithoutAttention, self).__init__()

        self.embedding_dim = 300
        self.n_hidden = 300
        self.n_class = 3
        self.l2_reg = 1e-5

        self.max_aspect_len = max_aspect_len
        self.max_context_len = 40
        self.embedding_matrix = embeddings_matrix
        self.embedding_matrix_target = embeddings_matrix_target

        self.aspect_lstm = tf.keras.layers.LSTM(self.n_hidden,
                                                return_sequences=True,
                                                recurrent_initializer='glorot_uniform',
                                                stateful=True)
        self.context_lstm = tf.keras.layers.LSTM(self.n_hidden,
                                                 return_sequences=True,
                                                 recurrent_initializer='glorot_uniform',
                                                 stateful=True)
        
        self.output_fc = tf.keras.layers.Dense(self.n_class, kernel_regularizer=tf.keras.regularizers.l2(l=self.l2_reg))

    
    def call(self, data, dropout=0.5):
        # print(1)
        aspects, contexts, labels, aspect_lens, context_lens = data

        aspect_inputs = tf.nn.embedding_lookup(self.embedding_matrix_target, aspects)
        aspect_inputs = tf.cast(aspect_inputs, tf.float32)
        # aspect_inputs = self.transpose(aspect_inputs)
        # print(1)
        context_inputs = tf.nn.embedding_lookup(self.embedding_matrix, contexts)
        context_inputs = tf.cast(context_inputs, tf.float32)
        # context_inputs = self.transpose(context_inputs)
        # print(context_inputs.shape, aspect_inputs.shape)
        aspect_outputs = self.aspect_lstm(aspect_inputs)
        aspect_outputs = tf.nn.max_pool1d(aspect_outputs, 3, 1, "SAME")
        aspect_outputs = tf.reduce_sum(aspect_outputs, 1)
        # print(aspect_avg.shape, aspect_outputs.shape)
        context_outputs = self.context_lstm(context_inputs)
        context_outputs = tf.nn.max_pool1d(context_outputs, 3, 1, "SAME")
        
        context_outputs = tf.reduce_sum(context_outputs, 1)
        # print(context_outputs1.shape)
        
        rep = tf.concat([aspect_outputs, context_outputs], 1)
        predict = self.output_fc(rep)
        
        return predict, labels, aspect_outputs, context_outputs

class IAN(tf.keras.Model):

    def __init__(self, embeddings_matrix=None, embeddings_matrix_target=None, max_aspect_len=10):
        super(IAN, self).__init__()

        self.embedding_dim = 300
        self.n_hidden = 300
        self.n_class = 3
        self.l2_reg = 1e-5

        self.max_aspect_len = max_aspect_len
        self.max_context_len = 40
        self.embedding_matrix = embeddings_matrix
        self.embedding_matrix_target = embeddings_matrix_target

        self.aspect_lstm = tf.keras.layers.LSTM(self.n_hidden,
                                                return_sequences=True,
                                                recurrent_initializer='glorot_uniform',
                                                stateful=True)
        self.context_lstm = tf.keras.layers.LSTM(self.n_hidden,
                                                 return_sequences=True,
                                                 recurrent_initializer='glorot_uniform',
                                                 stateful=True)
        rand_state = np.random.RandomState(42)
        self.aspect_w = rand_state.uniform(-.1, .1, [self.n_hidden, self.n_hidden])
        self.aspect_b = np.zeros([self.n_hidden]) 
        self.context_w = rand_state.uniform(-.1, .1, [self.n_hidden, self.n_hidden])
        self.context_b = np.zeros([self.n_hidden])
        
        self.output_fc = tf.keras.layers.Dense(self.n_class, kernel_regularizer=tf.keras.regularizers.l2(l=self.l2_reg))

    # def transpose(self, X):
    #   new_X = np.full((X.shape[0], X.shape[2], X.shape[1]), 0, dtype=float)
    #   for i in range(X.shape[0]):
    #     new_X[i] = tf.transpose(X[i])
    #   return new_X

    # def build(self, input_shape):
    #     self.aspect_w = self.add_weight(shape=(self.n_hidden, self.n_hidden),initializer="normal")
    #     self.aspect_b = self.add_weight(shape=(self.n_hidden,),initializer="zeros")
    #     self.context_w = self.add_weight(shape=(self.n_hidden, self.n_hidden),initializer="normal")
    #     self.context_b = self.add_weight(shape=(self.n_hidden,),initializer="zeros")        
    #     super(IAN, self).build(input_shape)

    def call(self, data, dropout=0.5):
        # print(1)
        aspects, contexts, labels, aspect_lens, context_lens = data

        aspect_inputs = tf.nn.embedding_lookup(self.embedding_matrix_target, aspects)
        aspect_inputs = tf.cast(aspect_inputs, tf.float32)
        # aspect_inputs = self.transpose(aspect_inputs)
        # print(1)
        context_inputs = tf.nn.embedding_lookup(self.embedding_matrix, contexts)
        context_inputs = tf.cast(context_inputs, tf.float32)
        # context_inputs = self.transpose(context_inputs)
        # print(context_inputs.shape, aspect_inputs.shape)
        aspect_outputs = self.aspect_lstm(aspect_inputs)
        aspect_outputs = tf.nn.max_pool1d(aspect_outputs, 3, 1, "SAME")
        aspect_avg = tf.reduce_mean(aspect_outputs, 1)
        # print(aspect_avg.shape, aspect_outputs.shape)
        context_outputs = self.context_lstm(context_inputs)
        context_outputs = tf.nn.max_pool1d(context_outputs, 3, 1, "SAME")
        context_avg = tf.reduce_mean(context_outputs, 1)
        # context_outputs1 = tf.reduce_sum(context_outputs, 1)
        # print(context_outputs1.shape)
        aspect_att = tf.nn.softmax(tf.nn.tanh(tf.einsum('ijk,kl,ilm->ijm', aspect_outputs, self.aspect_w,
                                                        tf.expand_dims(context_avg, -1)) + self.aspect_b), axis=1)
        
        aspect_rep = tf.reduce_sum(aspect_att * aspect_outputs, 1)
        
        context_att = tf.nn.softmax(tf.nn.tanh(tf.einsum('ijk,kl,ilm->ijm', context_outputs, self.context_w,
                                                         tf.expand_dims(aspect_avg, -1)) + self.context_b), axis=1)
        context_rep = tf.reduce_sum(context_att * context_outputs, 1)
        # print(context_rep.shape)
        rep = tf.concat([aspect_rep, context_rep], 1)
        predict = self.output_fc(rep)
        
        return predict, labels, aspect_att, context_att

def run(model, train_data, test_data, stopping_accuracy=0.70, run_only='both'):
    
    max_acc, step = 0., -1
    train_accuracy, test_accuracy, train_loss_list, test_loss_list = [], [], [], [] 
    train_data_size = len(train_data[0])
    train_data = tf.data.Dataset.from_tensor_slices(train_data)
    train_data = train_data.shuffle(buffer_size=train_data_size).batch(batch_size, drop_remainder=True)
    # print(1)
    test_data_size = len(test_data[0])
    test_data = tf.data.Dataset.from_tensor_slices(test_data)
    test_data = test_data.batch(batch_size, drop_remainder=True)
    # print(1)
    
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate) 
    
    # print(1)
    for i in range(n_epoch):
        start_time = time.time()
        cost, predict_list, labels_list = 0., [], []
        if run_only == 'both':
          for (iterator, data) in enumerate(train_data):
              
              # print(type(data))
              with tf.GradientTape() as tape:
                  predict, labels, aspect_att, context_att = model(data, dropout=0.5)
                  
                  loss_t = tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=labels)
                  loss = tf.reduce_mean(loss_t)
                  cost += tf.reduce_sum(loss_t)
                  # print(1)
              grads = tape.gradient(loss, model.variables)
              optimizer.apply_gradients(zip(grads, model.variables))
              predict_list.extend(tf.argmax(tf.nn.softmax(predict), 1).numpy())
              labels_list.extend(tf.argmax(labels, 1).numpy())
          train_acc, class_train_acc = evaluate(predict_list, labels_list)
          train_loss = cost / train_data_size
          train_accuracy.append(train_acc)
          train_loss_list.append(train_loss)

        cost, predict_list, labels_list = 0., [], []

        for (iterator, data) in enumerate(test_data):

            predict, labels, aspect_att1, context_att1 = model(data, dropout=0.5)
            # print(aspect_att1.shape)
            loss_t = tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=labels)
            cost += tf.reduce_sum(loss_t)
            predict_list.extend(tf.argmax(tf.nn.softmax(predict), 1).numpy())
            labels_list.extend(tf.argmax(labels, 1).numpy())
            # print(1)
        test_acc, class_test_acc = evaluate(predict_list, labels_list)
        test_loss = cost / test_data_size
        test_accuracy.append(test_acc)
        test_loss_list.append(test_loss)
        

        if test_acc  > max_acc :
            max_acc = test_acc
            step = i
            
        end_time = time.time()
        if run_only == 'both':
          print('Epoch {0:d}/{1:d}'.format(i+1,n_epoch), ": {0:.3f}sec".format((end_time - start_time)))
          print('===============>  train-loss=%.6f; train-acc=%.6f; validation-loss=%.6f; validation-acc=%.6f' % 
                (train_loss, train_acc, test_loss, test_acc))
          print('Classwise train accuracy:', class_train_acc) 
          print('Classwise test accuracy:', class_test_acc)
        
        else:
          print('Time: {0:.3f}sec'.format((end_time - start_time)))
          print('===============>  validation-loss=%.6f; validation-acc=%.6f' % 
                (test_loss, test_acc))
          print('Classwise test accuracy:', class_test_acc)
          break
        if test_acc > stopping_accuracy:
          break
    if run_only == 'both':
      print('\n\nThe max accuracy of testing results: acc %.3f of epoch %s' % (max_acc, step+1))
      return np.array(train_accuracy), np.array(test_accuracy), np.array(train_loss_list), np.array(test_loss_list)

"""Evaluate"""

def evaluate(y_pred, y_true):
    
    hit_count = np.zeros(3, dtype='int32')
    original_count = np.zeros(3, dtype='int32')
    n_test = len(y_true)

    for i in range(n_test):
        y_p = int(y_pred[i])
        y_g = y_true[i]
        original_count[y_g] += 1
        if y_p == y_g:
            hit_count[y_p] += 1

    total_hit = sum(hit_count)
    acc = float(total_hit) / n_test
    # print(hit_count[0])
    class_acc = []
    class_acc.append((hit_count[0] / original_count[0]))
    class_acc.append((hit_count[1] / original_count[1]))
    class_acc.append((hit_count[2] / original_count[2]))
    class_acc = np.round_(np.array(class_acc), decimals = 3)
    return acc, class_acc

def getting_attention_weights(X_test_padded, corpus_test, att_context, indx=0):
  tokenized_text = []
  attention_output = []
  for i in range(len(X_test_padded[indx])):
    try:
      if X_test_padded[indx][i] != 0:
        tokenized_text.append(corpus_test[indx].split()[i])
        attention_output.append(sum(att_context[indx][i]))
    except:
      pass
  attention_output = np.array(attention_output)
  attention_output1 = np.square(attention_output)
  sum_sqr = np.sqrt(np.sum(attention_output1))
  attention_output /= sum_sqr
  return tokenized_text, attention_output

class CharVal(object):
    def __init__(self, char, val):
        self.char = char
        self.val = val

    def __str__(self):
        return self.char

def rgb_to_hex(rgb):
    return '#%02x%02x%02x' % rgb
def color_charvals(s):
    r = 255-int(s.val*255)
    color = rgb_to_hex((255, r, r))
    return 'background-color: %s' % color

"""# Laptop"""

corpus_train = simplify(train_laptop['context'], train_laptop['output'])
corpus_test = simplify(test_laptop['context'], test_laptop['output'])

embeddings_matrix, X_train_padded, X_test_padded, vocab_size = preprocessing(corpus_train, corpus_test, laptop_glove)

le = LabelEncoder()
y_train_laptop = le.fit_transform(train_laptop['output'])
y_test_laptop = le.transform(test_laptop['output'])
num_classes = np.unique(y_train_laptop).shape[0]
y_train_laptop1 = np_utils.to_categorical(y_train_laptop, num_classes)
y_test_laptop1 = np_utils.to_categorical(y_test_laptop, num_classes)

print(embeddings_matrix.shape, X_train_padded.shape, X_test_padded.shape, y_train_laptop1.shape)

corpus_train_target = simplify(train_laptop['target'], train_laptop['output'])
corpus_test_target = simplify(test_laptop['target'], test_laptop['output'])

embeddings_matrix_target, X_train_padded_target, X_test_padded_target, vocab_size_target = preprocessing(corpus_train_target, corpus_test_target, laptop_glove_target, max_sentence_length=10)

print(embeddings_matrix_target.shape, X_train_padded_target.shape, X_test_padded_target.shape)

data = X_train_padded_target, X_train_padded, y_train_laptop1, np.full((X_train_padded.shape[0],), 10.), np.full((X_train_padded.shape[0],), 40.)
data_test = X_test_padded_target, X_test_padded, y_test_laptop1, np.full((X_test_padded.shape[0],), 10.), np.full((X_test_padded.shape[0],), 40.)

"""## WithoutAttention Model"""

batch_size = 64
learning_rate = 0.01
n_epoch = 10

model_1 = WithoutAttention(embeddings_matrix, embeddings_matrix_target)
train_accuracy, test_accuracy, train_loss, test_loss = run(model_1, data, data_test, stopping_accuracy=0.699)

train_accuracy, test_accuracy, train_loss, test_loss = run(model_1, data, data_test, stopping_accuracy=0.699)

"""## IAN Models"""

batch_size = 64
learning_rate = 0.01
n_epoch = 20

model_1 = IAN(embeddings_matrix, embeddings_matrix_target)
train_accuracy, test_accuracy, train_loss, test_loss = run(model_1, data, data_test, stopping_accuracy=0.699)

train_accuracy, test_accuracy, train_loss, test_loss = run(model_1, data, data_test, stopping_accuracy=0.7050)

train_accuracy, test_accuracy, train_loss, test_loss = run(model_1, data, data_test, stopping_accuracy=0.7150)

model_1.save('/content/drive/MyDrive/DL/Assignment_4/Q2_model_laptop', save_format="tf")

model1 = load_model('/content/drive/MyDrive/DL/Assignment_4/Q2_model_laptop')

run(model1, data, data_test, run_only='test')

data_test111 = X_test_padded_target[:64], X_test_padded[:64], y_test_laptop1[:64], np.full((64,), 15.), np.full((64,), 40.)
pred, lbl, att_aspect, att_context = model1(data_test111)

predict_list, labels_list = [], []
predict_list.extend(tf.argmax(tf.nn.softmax(pred), 1).numpy())
labels_list.extend(tf.argmax(lbl, 1).numpy())

"""### For 1st input, attention weights results"""

indx = 0

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""### For 2nd input, attention weights results"""

indx = 20

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""### For 3rd input, attention weights results"""

indx = 55

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""# Restaurant """

corpus_train = simplify(train_rest['context'], train_rest['output'])
corpus_test = simplify(test_rest['context'], test_rest['output'])

embeddings_matrix, X_train_padded, X_test_padded, vocab_size = preprocessing(corpus_train, corpus_test, rest_glove)

le = LabelEncoder()
y_train_rest = le.fit_transform(train_rest['output'])
y_test_rest = le.transform(test_rest['output'])
num_classes = np.unique(y_train_rest).shape[0]
y_train_rest1 = np_utils.to_categorical(y_train_rest, num_classes)
y_test_rest1 = np_utils.to_categorical(y_test_rest, num_classes)

print(embeddings_matrix.shape, X_train_padded.shape, X_test_padded.shape, y_train_laptop1.shape)

corpus_train_target = simplify(train_rest['target'], train_rest['output'])
corpus_test_target = simplify(test_rest['target'], test_rest['output'])

embeddings_matrix_target, X_train_padded_target, X_test_padded_target, vocab_size_target = preprocessing(corpus_train_target, corpus_test_target, rest_glove_target, max_sentence_length=15)

print(embeddings_matrix_target.shape, X_train_padded_target.shape, X_test_padded_target.shape)

data = X_train_padded_target, X_train_padded, y_train_rest1, np.full((X_train_padded.shape[0],), 10.), np.full((X_train_padded.shape[0],), 40.)
data_test = X_test_padded_target, X_test_padded, y_test_rest1, np.full((X_test_padded.shape[0],), 10.), np.full((X_test_padded.shape[0],), 40.)

"""## WithoutAttention Model"""

batch_size = 64
learning_rate = 0.01
n_epoch = 20

model_2 = WithoutAttention(embeddings_matrix, embeddings_matrix_target)
train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data, data_test, stopping_accuracy=0.780)

plotting_epochs(train_accuracy, test_accuracy, 'Accuracy-Restaurant')

"""## IAN Models"""

batch_size = 64
learning_rate = 0.01
n_epoch = 20

model_2 = IAN(embeddings_matrix, embeddings_matrix_target)
train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data, data_test, stopping_accuracy=0.770)

train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data, data_test, stopping_accuracy=0.780)

train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data, data_test, stopping_accuracy=0.775)

model_2.save('/content/drive/MyDrive/DL/Assignment_4/Q2_model_rest', save_format="tf")

model2 = load_model('/content/drive/MyDrive/DL/Assignment_4/Q2_model_rest')

run(model2, data, data_test, run_only='test')

data_test111 = X_test_padded_target[:64], X_test_padded[:64], y_test_laptop1[:64], np.full((64,), 15.), np.full((64,), 40.)
pred, lbl, att_aspect, att_context = model2(data_test111)

predict_list, labels_list = [], []
predict_list.extend(tf.argmax(tf.nn.softmax(pred), 1).numpy())
labels_list.extend(tf.argmax(lbl, 1).numpy())

"""### For 1st input, attention weights results"""

indx = 0

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""### For 2nd input, attention weights results"""

indx = 20

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""### For 3rd input, attention weights results"""

indx = 10

tokenized_text, attention_output = getting_attention_weights(X_test_padded, corpus_test, att_context.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

tokenized_text, attention_output = getting_attention_weights(X_test_padded_target, corpus_test_target, att_aspect.numpy(), indx)
char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_output)]
char_df = pd.DataFrame(char_vals).transpose()
char_df = char_df.style.applymap(color_charvals)
char_df

print(corpus_test[indx], "\n==>", corpus_test_target[indx], "\n==>", list_emotions[predict_list[indx]])

"""## OverSample"""

batch_size = 64
learning_rate = 0.01
n_epoch = 10

data11 = X_train_padded_target1, X_train_padded1, y_train_rest11, np.full((X_train_padded1.shape[0],), 15.), np.full((X_train_padded1.shape[0],), 40.)
data_test11 = X_test_padded_target, X_test_padded, y_test_rest1, np.full((X_test_padded.shape[0],), 15.), np.full((X_test_padded.shape[0],), 40.)

model_2 = IAN(embeddings_matrix, embeddings_matrix_target)
train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data11, data_test11)

plotting_epochs(train_accuracy, test_accuracy, 'Accuracy-Restaurant')
plotting_epochs(train_loss, test_loss, 'Loss-Restaurant')

train_accuracy, test_accuracy, train_loss, test_loss = run(model_2, data11, data_test11)

plotting_epochs(train_accuracy, test_accuracy, 'Accuracy-Restaurant')
plotting_epochs(train_loss, test_loss, 'Loss-Restaurant')