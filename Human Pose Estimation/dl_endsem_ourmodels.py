# -*- coding: utf-8 -*-
"""DL_Endsem_OurModels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eVnkVPiSUYUaGpaFb2jMiRWecUtDbqnO
"""

!nvidia-smi

"""# Mount the drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Load the libraries"""

import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sn
from google.colab import files
from scipy.io import loadmat
import io
import joblib
import tensorflow as tf
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
# import wfdb
from sklearn.metrics import confusion_matrix
import seaborn as sns
import ast
from keras import regularizers
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from imblearn.under_sampling import RandomUnderSampler, TomekLinks 
from imblearn.over_sampling import RandomOverSampler, SMOTE
from keras import layers
from keras.models import Model
from keras.layers import Input
from keras.layers import Conv2D, BatchNormalization, Flatten, Dense, Convolution2D
from keras.layers import MaxPool2D, ZeroPadding2D, MaxPooling2D
from keras.layers import Dropout
from keras.layers.merge import concatenate
from keras.utils import plot_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow import keras
from keras.utils import np_utils
from keras.models import Sequential, load_model
from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax
from keras import regularizers
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import ImageDataGenerator
from numpy import expand_dims
import matplotlib.image as mpimg 
from keras.applications.resnet50 import ResNet50
from scipy import io as spio
import matplotlib.colors as mcolors
from tensorflow.keras.applications.vgg16 import VGG16
from keras.preprocessing import image
import matplotlib.pyplot as plt

"""# Helper functions"""

def plotting_epochs(training_, validation_, lossOrAccu, figsize=(6,4)):
  plt.figure(figsize=figsize)
  plt.plot(training_, 'black', linewidth=2.0)
  plt.plot(validation_, 'blue', linewidth=2.0)
  plt.legend(['Training '+lossOrAccu, 'Validation '+lossOrAccu], fontsize=14)
  plt.xlabel('Epochs', fontsize=10)
  plt.ylabel(lossOrAccu, fontsize=10)
  plt.title(lossOrAccu+' Curves', fontsize=12)

def heatMap(y_test, y_pred, figsize):
  log_cm = confusion_matrix(np.array(y_test).astype('int'), y_pred.astype('int'))

  f, p = plt.subplots(figsize=figsize)
  sns.heatmap(log_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=p, cmap="YlGnBu")
  plt.title('Confusion Matrix', size=16)
  plt.xlabel('Predicted Labels', size=14)
  plt.ylabel('Original Labels', size=14)
  plt.show()

def feature_maps_plotting(feature_maps, x_axis, y_axis=None):
  
  if y_axis == None:
    y_axis = x_axis
  ix = 1
  plt.figure(figsize=(20,20))
  for _ in range(x_axis):
    for _ in range(y_axis):
      
      ax = plt.subplot(x_axis, y_axis, ix)
      plt.axis('off')
      plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')
      ix += 1
  plt.show()

"""# Data Analysis"""

df=pd.read_csv('/content/drive/MyDrive/DL/DL_Project_Data/For_Data_Stats.csv')
df

Category_df = pd.DataFrame(df['Category'].value_counts().sort_values(), columns=['index', 'Category'])
Activity_df = pd.DataFrame(df['Activity'].value_counts().sort_values(), columns=['index', 'Activity'])
plt.figure(figsize = (12,7)) 
plt.bar(Category_df.index, Category_df.Category, color=('tab:blue','crimson','tab:green','tab:grey','tab:cyan','tab:pink','tab:olive','tab:blue','tab:red','tab:purple','tab:brown','lightcoral','greenyellow','teal','tab:orange','navy','salmon'))
plt.xticks([])
plt.title('Category wise Distribution in Reduced Data')  
plt.xlabel(str(len(df['Category'].value_counts())) + ' Categories')
plt.ylabel('Images per Class')
plt.show()

plt.figure(figsize = (12,7)) 
plt.bar(Activity_df.index, Activity_df.Activity, color=(mcolors.TABLEAU_COLORS))
plt.xticks([])
plt.title('Activity wise Distribution in Reduced Data')  
plt.xlabel(str(len(df['Activity'].value_counts())) + ' Activities')
plt.ylabel('Images per Class')
plt.show()

frequencies = df['Category'].value_counts().sort_values()
freq_series = pd.Series(frequencies)
labels = df['Category'].value_counts().sort_index().index
plt.figure(1, figsize = (12,8))
ax = freq_series.plot(kind='barh', color=(mcolors.TABLEAU_COLORS), width=0.9)
ax.set_ylabel('Classes')
ax.set_yticklabels(labels)
ax.set_title('Category wise Distribution in Original Data')
ax.set_xlabel('Images per Class')
col = ax.patches

for c in col:
    x_value = c.get_width()
    y_value = c.get_y() + c.get_height() / 2
    clsses = "{:.2f}".format(x_value/15000)
    plt.annotate(clsses, (x_value, y_value), xytext=(5, 0), textcoords="offset points")

mat_data = spio.loadmat('/content/drive/MyDrive/DL/mpii_human_pose_v1_u12_1.mat')
original_data = pd.DataFrame(index=range(0,len(mat_data['RELEASE'][0][0][0][0])),columns=['Image_name','Category','Activity'])
for i in range(0,len(mat_data['RELEASE'][0][0][0][0])):
  original_data.loc[[i,0],'Image_name'] = mat_data["RELEASE"]["annolist"][0,0][0][i]['image']['name'][0,0][0]
  try: original_data.loc[[i,1],'Category'] = mat_data["RELEASE"]["act"][0,0][:,0][i]["cat_name"][0]
  except Exception: pass
  try: original_data.loc[[i,2],'Activity'] = mat_data["RELEASE"]["act"][0,0][:,0][i]["act_name"][0]
  except Exception: pass
original_data.head()

Category_original = pd.DataFrame(original_data['Category'].value_counts().sort_values(), columns=['index', 'Category'])
Activity_original = pd.DataFrame(original_data['Activity'].value_counts().sort_values(), columns=['index', 'Activity'])

plt.figure(figsize = (12,7))   
plt.bar(Category_original.index, Category_original.Category, color=(mcolors.TABLEAU_COLORS))
plt.xticks([])
plt.title('Category wise Distribution in Original Data')  
plt.xlabel(str(len(original_data['Category'].value_counts())) + ' Categories')
plt.ylabel('Images per Class')
plt.show()
plt.close()  

plt.figure(figsize = (12,7))  
plt.bar(Activity_original.index, Activity_original.Activity, color=(mcolors.CSS4_COLORS))
plt.xticks([])
plt.title('Activity wise Distribution in Original Data')  
plt.xlabel(str(len(original_data['Activity'].value_counts())) + ' Activities')
plt.ylabel('Images per Class')

plt.show()

frequencies = original_data['Category'].value_counts().sort_values()
freq_series = pd.Series(frequencies)
labels = original_data['Category'].value_counts().sort_index().index
plt.figure(1, figsize = (12,8))
ax = freq_series.plot(kind='barh', color=(mcolors.TABLEAU_COLORS), width=0.9)
ax.set_ylabel('Classes')
ax.set_yticklabels(labels)
ax.set_title('Category wise Distribution in Original Data')
ax.set_xlabel('Images per Class')
col = ax.patches

for c in col:
    x_value = c.get_width()
    y_value = c.get_y() + c.get_height() / 2
    clsses = "{:.2f}".format(x_value/15000)
    plt.annotate(clsses, (x_value, y_value), xytext=(5, 0), textcoords="offset points")

"""# Data Augmentation"""

img_size = 224
train_datagen = ImageDataGenerator(rescale = 1./255)

train_generator = train_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/train1',
                                                    target_size = (img_size,img_size),
                                                    batch_size=30, 
                                                    class_mode="sparse",
                                                    subset = "training")

train_generator = train_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/DL_Project_Data/train1',
                                                    target_size = (img_size,img_size),
                                                    batch_size=30, 
                                                    class_mode="sparse",
                                                    subset = "training")

validation_datagen = ImageDataGenerator(rescale = 1./255, validation_split = .9999)

validation_generator = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/test_1',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",
                                                              subset = "validation")

validation_generator = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/DL_Project_Data/test_1',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",
                                                              subset = "validation")

validation_generator1 = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/test_1',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",shuffle=False,
                                                              subset = "validation")

validation_generator1 = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/DL_Project_Data/test_1',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",shuffle=False,
                                                              subset = "validation")

train_generator50 = train_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/train50',
                                                    target_size = (img_size,img_size),
                                                    batch_size=30, 
                                                    class_mode="sparse",
                                                    subset = "training")

validation_generator50 = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/test50',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",
                                                              subset = "validation")

validation_generator50_1 = validation_datagen.flow_from_directory(directory = '/content/drive/MyDrive/DL/test50',
                                                              target_size = (img_size,img_size),
                                                              batch_size=30, class_mode="sparse",shuffle=False,
                                                              subset = "validation")

"""# 20 & 14 Classification Data"""

poses = {9:'Excercising & cardio in a gym', 
         17:'Dancing & its different forms',
         18:'Couple dance (salsa)',
         23:'Bicycle riding',
         24:'Bicycle racing',
         43:'Carpentering',
         58:'Excercise & weight training',
         70:'Cooking',
         123:'Cutting trees (Foresting)',
         130:'Golf',
         248:'Gymming & cardio',
         252:'Climbing',
         256:'Skipping',
         257:'Gymming & back-body training',
         290:'Skating',
         300:'Skiing',
         313:'Football, playing in a field',
         314:'Baseball',
         364:'Cardio',
         396:'Stretching & Flexing'}

combine_class = {'Excercising & cardio in a gym':0, 'Excercise & weight training':0, 'Gymming & cardio':0, 'Gymming & back-body training':0,
                 'Cardio':0, 'Dancing & its different forms':1, 'Couple dance (salsa)':1, 'Bicycle riding':2, 'Bicycle racing':2, 
                 'Carpentering':3, 'Cooking':4, 'Cutting trees (Foresting)':5, 'Golf':6, 'Climbing':7, 'Skipping':8, 'Skating':9,
                 'Skiing':10, 'Football, playing in a field':11, 'Baseball':12, 'Stretching & Flexing':13}

class_labels = {0:'Excercising & cardio', 1:'Dancing', 2:'Bicycling', 3:'Carpentering', 4:'Cooking', 5:'Cutting trees (Foresting)',
                6:'Golf', 7:'Climbing', 8:'Skipping', 9:'Skating', 10:'Skiing', 11:'Football', 12:'Baseball', 13:'Stretching & Flexing'}

X_train = joblib.load("/content/drive/MyDrive/DL/DL_Project_Data/TrainData_Classification_20")
X_test = joblib.load("/content/drive/MyDrive/DL/DL_Project_Data/TestData_Classification_20")
y_train = joblib.load("/content/drive/MyDrive/DL/DL_Project_Data/TrainData_Classification_20_labels")
y_test = joblib.load("/content/drive/MyDrive/DL/DL_Project_Data/TestData_Classification_20_labels")

for i in range(len(y_train)): # Combining common labels
  y_train[i] = combine_class[y_train[i]]
for i in range(len(y_test)):
  y_test[i] = combine_class[y_test[i]]

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

list_poses = le.classes_
print(list_poses)
print(len(list_poses))

def shuffling(X, y):
  data = pd.DataFrame(X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3]))
  data['label'] = y
  data = data.sample(frac=1, random_state=42)
  X = data.iloc[:, :-1].values.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3])
  y = data.iloc[:, -1].values
  return X, y

X_train1, y_train1 = shuffling(np.array(X_train), y_train)

X_test, y_test = shuffling(np.array(X_test), y_test)

num_classes = np.unique(y_train1).shape[0]
y_train11 = np_utils.to_categorical(y_train1, num_classes)
y_test1 = np_utils.to_categorical(y_test, num_classes)

"""# 50 classes"""

X_train = joblib.load('/content/drive/MyDrive/DL/DL_Project_Data/Regression_train_50')
y_train = joblib.load('/content/drive/MyDrive/DL/DL_Project_Data/Regression_train_IDS_50')
X_test = joblib.load('/content/drive/MyDrive/DL/DL_Project_Data/Regression_test_50')
y_test = joblib.load('/content/drive/MyDrive/DL/DL_Project_Data/Regression_test_IDS_50')

data = pd.read_csv('/content/drive/MyDrive/DL/mpii_dataset.csv')
data

np.unique(data.iloc[:, -1].values).size

image_name = data.iloc[:, 1].values
image_lbl = data.iloc[:, -2].values
image_cat = data.iloc[:, -1].values

image_lbl[17370], image_cat[17370]

y_train.shape

def take_Labels_Categories(y):
  y1 = []
  for i in range(len(y)):
    for j in range(len(image_name)):
      if y[i] == image_name[j]:
        # y1.append((image_lbl[j], image_cat[j]))
        y1.append(image_lbl[j])
        break
  return np.array(y1)

# y_train12 = take_Labels_Categories(y_train)
# y_test12 = take_Labels_Categories(y_test)

y_train1 = take_Labels_Categories(y_train)

y_test1 = take_Labels_Categories(y_test)

len(y_train), len(y_train1), len(y_test), len(y_test1)

np.unique(y_train1).size

# y_all = np.concatenate((y_train12, y_test12))

# data1 = pd.DataFrame(y_all, columns=['Activity', 'Category'])
# data1['ImageName'] = np.concatenate((y_train, y_test))
# data1.head()

# data1.to_csv('/content/drive/MyDrive/DL/DL_Project_Data/For_Data_Stats.csv')

plt.imshow(X_test[110])
print(y_test1[110])

le = LabelEncoder()
y_train1 = le.fit_transform(y_train1)
y_test1 = le.transform(y_test1)

class_labels = le.classes_

num_classes = np.unique(y_train1).shape[0]
y_train11 = np_utils.to_categorical(y_train1, num_classes)
y_test11 = np_utils.to_categorical(y_test1, num_classes)

"""## Creating Labels For Regression"""

data1 = data.iloc[:,:].values

cl1 = data1[:, 1]
cl2 = data1[:, -2]

data1 = data1[:, 2:-3]

def create_labels(y_train):
  y = []
  for j in range(len(y_train)):
    for i in range(len(cl1)):
      if y_train[j] == cl1[i]:
        y.append(data1[i])
        break
  return np.array(y)

y_train1 = create_labels(y_train)
y_test1 = create_labels(y_test)

y_test1.shape, y_train1.shape

y_test1 = y_test1.astype('float')
y_train1 = y_train1.astype('float')

"""# Xception Model On 20 Classes"""

Xception = keras.applications.xception.Xception(include_top=False, weights='imagenet', pooling='avg', classes=20)
Flatten  = keras.layers.Flatten()
Output   = keras.layers.Dense(20, activation="softmax")

XModel = tf.keras.Sequential([ Xception, Flatten, Output ])

for layer in Xception.layers[:-7]:
    layer.trainable = False

XModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = XModel.fit(train_generator, epochs=10, validation_data=validation_generator)

history1 = XModel.fit(train_generator, epochs=10, validation_data=validation_generator)

XModel.save('/content/drive/MyDrive/DL/Xception_On20_Classes.h5')

poses = {9:'Excercising & cardio in a gym', 
         17:'Dancing & its different forms',
         18:'Couple dance (salsa)',
         23:'Bicycle riding',
         24:'Bicycle racing',
         43:'Carpentering',
         58:'Excercise & weight training',
         70:'Cooking',
         123:'Cutting trees (Foresting)',
         130:'Golf',
         248:'Gymming & cardio',
         252:'Climbing',
         256:'Skipping',
         257:'Gymming & back-body training',
         290:'Skating',
         300:'Skiing',
         313:'Football, playing in a field',
         314:'Baseball',
         364:'Cardio',
         396:'Stretching & Flexing'}

"""## Predicting by Xception model"""

XModel = load_model('/content/drive/MyDrive/DL/DL_Project_Data/Human_Pose_Models/Our Models/Xception_On20_Classes.h5')

pred=XModel.predict_generator(validation_generator1)
predicted_class_indices=np.argmax(pred,axis=1)
labels = (validation_generator1.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

plabel=[]
for i in predictions:
  for m,n in poses.items():
    m=str(m)
    if m==i:
      plabel.append(n)

"""## Testing Xception """

y_true = validation_generator1.classes
truth = []
pred = []

for i in range(30):
    x, y = next(validation_generator1)
    pred.append(XModel.predict(x))
    truth.append(y) 

pred = np.concatenate(pred)
truth = np.concatenate(truth)
predicted_class=np.argmax(pred,axis=1)
print(confusion_matrix(truth,predicted_class))

def heatMap(y_test, y_pred, figsize):
  log_cm = confusion_matrix(np.array(y_test).astype('int'), y_pred.astype('int'))

  f, p = plt.subplots(figsize=figsize)
  sns.heatmap(log_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=p, cmap="YlGnBu")
  plt.title('Confusion Matrix', size=16)
  plt.xlabel('Predicted Labels', size=14)
  plt.ylabel('Original Labels', size=14)
  plt.show()

heatMap(truth,predicted_class,(16,16))

"""## Visualizing Results of Xception model"""

x,y = validation_generator1[0] #for 1st batch
for i in range(0,30):
    image = x[i]
    label = y[i]
    print(plabel[i])
    plt.imshow(image)
    plt.show()

from keras.preprocessing import image
import matplotlib.pyplot as plt
count=0
for ii in range(5):  #for first 5 batches
  x,y = validation_generator1[ii]
  for i in range(0,30):
      image = x[i]
      label = y[i]
      print(plabel[count])
      count+=1
      plt.imshow(image)
      plt.show()

"""# Xception Model on 50 CLaases"""

Xception = keras.applications.xception.Xception(include_top=False, weights='imagenet', pooling='avg', classes=50)
Flatten  = keras.layers.Flatten()
Output   = keras.layers.Dense(50, activation="softmax")

XModel = tf.keras.Sequential([ Xception, Flatten, Output ])

for layer in Xception.layers[:-7]:
    layer.trainable = False

XModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = XModel.fit(train_generator, epochs=10, validation_data=validation_generator)

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

XModel.save('/content/drive/MyDrive/DL/Xception_On50_Classes.h5')

Xmodel50 = load_model('/content/drive/MyDrive/DL/Xception_On50_Classes.h5')

history_50 = Xmodel50.fit(train_generator50, epochs=10, validation_data=validation_generator50)

pred=Xmodel50.predict_generator(validation_generator50_1)
predicted_class_indices=np.argmax(pred,axis=1)
labels = (validation_generator50_1.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

y_true = validation_generator50_1.classes
truth = []
pred = []

for i in range(30):
    x, y = next(validation_generator50_1)
    pred.append(Xmodel50.predict(x))
    truth.append(y) 

pred = np.concatenate(pred)
truth = np.concatenate(truth)
predicted_class=np.argmax(pred,axis=1)
print(confusion_matrix(truth,predicted_class))

heatMap(truth,predicted_class,(16,16))

"""# Xception on 14 class"""

Xception = keras.applications.xception.Xception(include_top=False, weights='imagenet', pooling='avg', classes=14)
Flatten  = keras.layers.Flatten()
Output   = keras.layers.Dense(14, activation="softmax")

XModel = tf.keras.Sequential([ Xception, Flatten, Output ])

for layer in Xception.layers[:-7]:
    layer.trainable = False

XModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = XModel.fit(X_train1,y_train11, epochs=20, validation_data=(X_test,y_test1))

history1 = XModel.fit(X_train1,y_train11, epochs=10, validation_data=(X_test,y_test1))

history2 = XModel.fit(X_train1,y_train11, epochs=10, validation_data=(X_test,y_test1))

history3 = XModel.fit(X_train1,y_train11, epochs=10, validation_data=(X_test,y_test1))

history4 = XModel.fit(X_train1,y_train11, epochs=10, validation_data=(X_test,y_test1))

XModel.save('/content/drive/MyDrive/DL/xception_On14_Classes.h5')
#XModel = load_model('/content/drive/MyDrive/DL/xception_On14_Classes.h5')

history5 = XModel.fit(X_train1,y_train11, epochs=1, validation_data=(X_test,y_test1))

"""# CNN Architecture

## ***20 class***

## CNN-1
"""

model = Sequential()

model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.3))

model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
# model.add(layers.Dense(1024, activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(layers.Dropout(0.5))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation='softmax'))

model.summary()

model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history_1.history['accuracy'], history_1.history['val_accuracy'], 'Accuracy')
plotting_epochs(history_1.history['loss'], history_1.history['val_loss'], 'Loss')

history = model.fit(X_train1, y_train11, batch_size=32, epochs = 80, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')
predictions = model.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))

model.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_Basic.h5')

"""## CNN-2"""

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.3))

model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.5))

model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.5))

model_1.add(layers.Flatten())
# model_1.add(layers.Dense(1024, activation='relu'))
# model_1.add(layers.BatchNormalization())
# model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(num_classes, activation='softmax'))

model_1.summary()

model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')
predictions = model_11.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))

# model_1.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_20_class.h5')

model_11 = load_model('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_20_class.h5')

model_11.evaluate(X_test, y_test1)

"""## CNN-3"""

def CNN_Block(model, num_filters, filter_size, padding, activation, pool_size, dropout):
  model.add(layers.Conv2D(num_filters, filter_size, padding=padding, activation=activation))
  model.add(layers.BatchNormalization())
  model.add(layers.Conv2D(num_filters, filter_size, padding=padding, activation=activation))
  model.add(layers.BatchNormalization())
  model.add(layers.MaxPooling2D(pool_size=pool_size))
  model.add(layers.Dropout(dropout))
  return model

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())

model_1 = CNN_Block(model_1, 64, (5, 5), 'same', 'relu', (3, 3), 0.6)
model_1 = CNN_Block(model_1, 128, (4, 4), 'same', 'relu', (3, 3), 0.6)
model_1 = CNN_Block(model_1, 256, (3, 3), 'same', 'relu', (2, 2), 0.6)
model_1 = CNN_Block(model_1, 512, (3, 3), 'same', 'relu', (2, 2), 0.6)

model_1.add(layers.Flatten())
model_1.add(layers.Dense(512, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(num_classes, activation='softmax'))

model_1.summary()

model_1.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 30, validation_data=(X_test, y_test1))

model_1.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_Modified_1.h5')

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')
predictions = model_1.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))



"""## ***14 Class***

## CNN-1
"""

model = Sequential()

model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.3))

model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
# model.add(layers.Dense(1024, activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(layers.Dropout(0.5))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation='softmax'))

model.summary()

model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

"""## CNN-2"""

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.5))

model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.6))

model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.7))

model_1.add(layers.Flatten())
# model_1.add(layers.Dense(1024, activation='relu'))
# model_1.add(layers.BatchNormalization())
# model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.6))
model_1.add(layers.Dense(num_classes, activation='softmax'))

model_1.summary()

model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 50, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

predictions = model_1.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))

history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')
predictions = model_1.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))

model_1.fit(X_train1, y_train11, batch_size=32, epochs = 1, validation_data=(X_test, y_test1))

model_1.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_14_Classes.h5')

model_11 = load_model('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_14_Classes.h5')

model_11.evaluate(X_test, y_test1)

predictions = model_11.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test, y_pred, (16, 10))
print('F1-Score:',metrics.f1_score(y_test, y_pred, average='micro'))

index_good = []
index_bad = []
for i in range(len(y_test)):
  if (y_test[i] == y_pred[i]):
    index_good.append(i)
  else:
    index_bad.append(i)

len(index_bad), len(index_good)

"""### Miss-Classification results"""

indx = 2
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_bad[indx]]])

indx = 140
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_bad[indx]]])

indx = 15
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_bad[indx]]])

indx = 50
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_bad[indx]]])

"""### Feature Maps"""

indx_layer = 0
for layer in model_11.layers:
  if 'conv' in layer.name:
    filters, biases = layer.get_weights()
    print(indx_layer, layer.name, filters.shape)
  indx_layer += 1

img = np.array(X_test[index_good[indx]], ndmin=4)
img.shape

model1 = Model(inputs=model_11.inputs, outputs=model_11.layers[2].output)
model1.summary()

feature_maps = model1.predict(img)

feature_maps_plotting(feature_maps, 4, 4)

"""### Correct-Classification results"""

indx = 2
plt.imshow(X_test[index_good[indx]])
plt.axis('off')
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_good[indx]]])

indx = 140
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_good[indx]]])

indx = 200
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_good[indx]]])

indx = 500
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test[index_good[indx]]])

"""## CNN-3"""

def CNN_Block(model, num_filters, filter_size, padding, activation, pool_size, dropout):
  model.add(layers.Conv2D(num_filters, filter_size, padding=padding, activation=activation))
  model.add(layers.BatchNormalization())
  model.add(layers.Conv2D(num_filters, filter_size, padding=padding, activation=activation))
  model.add(layers.BatchNormalization())
  model.add(layers.MaxPooling2D(pool_size=pool_size))
  model.add(layers.Dropout(dropout))
  return model

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())

model_1 = CNN_Block(model_1, 64, (5, 5), 'same', 'relu', (3, 3), 0.6)
model_1 = CNN_Block(model_1, 128, (4, 4), 'same', 'relu', (3, 3), 0.6)
model_1 = CNN_Block(model_1, 256, (3, 3), 'same', 'relu', (2, 2), 0.6)
model_1 = CNN_Block(model_1, 512, (3, 3), 'same', 'relu', (2, 2), 0.6)

model_1.add(layers.Flatten())
model_1.add(layers.Dense(512, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(num_classes, activation='softmax'))

model_1.summary()

model_1.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_1.fit(X_train1, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test1))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

model_1.fit(X_train1, y_train11, batch_size=32, epochs = 1, validation_data=(X_test, y_test1))

"""## ***50 Class***

## CNN-1
"""

model = Sequential()

model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.3))

model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
# model.add(layers.Dense(1024, activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(layers.Dropout(0.5))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation='softmax'))

model.summary()

model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model.fit(X_train, y_train11, batch_size=32, epochs = 20, validation_data=(X_test, y_test11))

plotting_epochs(history_1.history['accuracy'], history_1.history['val_accuracy'], 'Accuracy')
plotting_epochs(history_1.history['loss'], history_1.history['val_loss'], 'Loss')

"""## CNN-2"""

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.5))

model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.6))

model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.7))

model_1.add(layers.Flatten())
# model_1.add(layers.Dense(1024, activation='relu'))
# model_1.add(layers.BatchNormalization())
# model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.6))
model_1.add(layers.Dense(num_classes, activation='softmax'))

model_1.summary()

model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_1.fit(X_train, y_train11, batch_size=32, epochs = 50, validation_data=(X_test, y_test11))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

predictions = model_1.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test1, y_pred, (18, 12))
print('F1-Score:',metrics.f1_score(y_test1, y_pred, average='micro'))

model_1.fit(X_train, y_train11, batch_size=32, epochs = 1, validation_data=(X_test, y_test11))

model_1.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_50_Classes.h5')

model_11 = load_model('/content/drive/MyDrive/DL/DL_Project_Data/CNN_2_50_Classes.h5')

model_11.evaluate(X_test, y_test11)

predictions = model_11.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test1, y_pred, (18, 12))
print('F1-Score:',metrics.f1_score(y_test1, y_pred, average='micro'))

index_good = []
index_bad = []
for i in range(len(y_test1)):
  if (y_test1[i] == y_pred[i]):
    index_good.append(i)
  else:
    index_bad.append(i)

len(index_bad), len(index_good)

"""### Miss-Classification results"""

indx = 2
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_bad[indx]]])

indx = 140
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_bad[indx]]])

indx = 15
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_bad[indx]]])

indx = 50
plt.imshow(X_test[index_bad[indx]])
print('Predicted Pose:',class_labels[y_pred[index_bad[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_bad[indx]]])

"""### Correct-Classification results"""

indx = 2
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_good[indx]]])

indx = 140
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_good[indx]]])

indx = 200
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_good[indx]]])

indx = 500
plt.imshow(X_test[index_good[indx]])
print('Predicted Pose:',class_labels[y_pred[index_good[indx]]]) 
print('Actual Pose:',class_labels[y_test1[index_good[indx]]])

"""## CNN-3"""

model_2 = Sequential()

model_2.add(layers.Conv2D(128, (5,5), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_2.add(layers.BatchNormalization())
model_2.add(layers.Conv2D(128, (5,5), padding='same', activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.MaxPooling2D(pool_size=(3,3)))
model_2.add(layers.Dropout(0.5))

model_2.add(layers.Conv2D(256, (5,5), padding='same', activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.Conv2D(256, (5,5), padding='same', activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.MaxPooling2D(pool_size=(3,3)))
model_2.add(layers.Dropout(0.6))

model_2.add(layers.Conv2D(512, (5,5), padding='same', activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.Conv2D(512, (5,5), padding='same', activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.MaxPooling2D(pool_size=(3,3)))
model_2.add(layers.Dropout(0.7))

model_2.add(layers.Flatten())
# model_2.add(layers.Dense(1024, activation='relu'))
# model_2.add(layers.BatchNormalization())
# model_2.add(layers.Dropout(0.5))
model_2.add(layers.Dense(512, activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.Dropout(0.6))
model_2.add(layers.Dense(num_classes, activation='softmax'))

model_2.summary()

model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = model_2.fit(X_train, y_train11, batch_size=32, epochs = 50, validation_data=(X_test, y_test11))

plotting_epochs(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy')
plotting_epochs(history.history['loss'], history.history['val_loss'], 'Loss')

predictions = model_2.predict(X_test)
y_pred = np.argmax(predictions, axis = 1)
heatMap(y_test1, y_pred, (18, 12))
print('F1-Score:',metrics.f1_score(y_test1, y_pred, average='micro'))

"""# Regression

# CNN-2
"""

model_1 = Sequential()

model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.5))

model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.6))

model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Conv2D(256, (3,3), padding='same', activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.MaxPooling2D(pool_size=(3,3)))
model_1.add(layers.Dropout(0.7))

model_1.add(layers.Flatten())
model_1.add(layers.Dense(512, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.5))
model_1.add(layers.Dense(256, activation='relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.6))
model_1.add(layers.Dense(32, activation='linear'))

model_1.summary()

model_1.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
history = model_1.fit(X_train, y_train1, epochs=50, batch_size=32, validation_data=(X_test, y_test1))

plotting_epochs(history.history['loss'], history.history['val_loss'], "Loss")

history = model_1.fit(X_train, y_train1, epochs=50, batch_size=32, validation_data=(X_test, y_test1))

plotting_epochs(history.history['loss'], history.history['val_loss'], "Loss")

# model_1.save('/content/drive/MyDrive/DL/DL_Project_Data/CNN_Model50_Regression.h5')

"""# PCP and PCKh metrics"""

def limb_lengths(y_true):
  points = [4, 5, 0, 1, 6, 7, 10, 11, 24, 25, 20, 21, 26, 27, 30, 31]
  i = 0
  ave_length = 0
  while i<16:
    num1 = abs(y_true[points[i]] - y_true[points[i+2]])
    i+=1
    num2 = abs(y_true[points[i]] - y_true[points[i+2]])
    ave_length += ((num1 + num2)/2)
    i+=3
  return ave_length

def PCP(y_test, y_pred):
  correctly_classified = 0
  for j in range(y_test.shape[0]):
    count = 0
    limb_lnth = limb_lengths(y_test[j])
    i=0
    while i < 32:
      num1 = y_pred[j][i]
      num2 = y_test[j][i]
      dist_x = abs(num1 - num2)
      i+=1
      num1 = y_pred[j][i]
      num2 = y_test[j][i]
      dist_y = abs(num1 - num2)
      i+=1
      joint_distance = dist_x + dist_y
      if joint_distance <= limb_lnth:
        count += 1
    if count > 8:
      correctly_classified += 1
  return correctly_classified/y_test.shape[0]

def Torso_diameter(y_true): #The torso diameter is defined as the distance between left shoulder and right hip of each ground-truth pose
  num1 = np.square(y_true[26] - y_true[4])
  num2 = np.square(y_true[27] - y_true[5])
  return np.sqrt(num1 + num2)

def Head_Norm(y_true, y_pred):
  num1 = np.square(y_true[18] - y_pred[18])
  num2 = np.square(y_true[19] - y_pred[19])
  return np.sqrt(num1 + num2)

def PCK(y_test, y_pred, scheme):
  correctly_classified = 0
  for j in range(y_test.shape[0]):
    count = 0
    diameter = 150
    if scheme=='head_norm':
      diameter = Head_Norm(y_test[j], y_pred[j])
    elif scheme=='torso':
      diameter = Torso_diameter(y_test[j])
    # print(diameter)
    i=0
    while i < 32:
      num1 = y_pred[j][i]
      num2 = y_test[j][i]
      dist_x = np.square(num1 - num2)
      i+=1
      num1 = y_pred[j][i]
      num2 = y_test[j][i]
      dist_y = np.square(num1 - num2)
      i+=1
      joint_distance = np.sqrt((dist_x + dist_y))
      if joint_distance <= diameter:
        count += 1
    if count >= 2:
      correctly_classified += 1
  return correctly_classified/y_test.shape[0]

"""## CNN results"""

reg_model = load_model('/content/drive/MyDrive/DL/DL_Project_Data/Human_Pose_Models/Regression_Models/Regression_Baseline_Midsem.h5')

y_pred1 = reg_model.predict(X_test)

pck = PCK(y_test1, y_pred1, 'head_norm')
print('PCKh for the proposed CNN model is: {0:.3f}%'.format(pck*100))

pcp = PCP(y_test1, y_pred1)
print('PCP for the proposed CNN model is: {0:.3f}%'.format(pcp*100))